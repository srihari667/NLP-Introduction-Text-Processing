{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##NLP Introduction & Text Processing"
      ],
      "metadata": {
        "id": "XrN61XJrA-Q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is Computational Linguistics and how does it relate to NLP?\n",
        "\n",
        "  **Computational Linguistics (CL)** is an interdisciplinary field that combines **linguistics, computer science, and artificial intelligence** to study and model **human language using computational methods**. Its main goal is to understand how language works and to create **formal, rule-based or statistical models** that represent linguistic structure such as syntax, semantics, morphology, and pragmatics.\n",
        "\n",
        "  **Natural Language Processing (NLP)** is an **application-oriented subfield** that focuses on enabling computers to **process, understand, and generate human language** in practical systems.\n",
        "\n",
        "  ### **Relationship between Computational Linguistics and NLP**\n",
        "\n",
        "  | Aspect   | Computational Linguistics           | Natural Language Processing      |\n",
        "  | -------- | ----------------------------------- | -------------------------------- |\n",
        "  | Focus    | Theoretical + modeling of language  | Practical language applications  |\n",
        "  | Approach | Linguistic rules, grammar, theory   | Algorithms, ML, deep learning    |\n",
        "  | Goal     | Understand language computationally | Build language-enabled systems   |\n",
        "  | Output   | Language models, grammars, theories | Chatbots, translators, analyzers |\n",
        "\n",
        "\n",
        "  ### **Examples**\n",
        "\n",
        "  * CL helps define **grammar rules** and **semantic structures**\n",
        "    * NLP uses those rules to build:\n",
        "    * Machine Translation (Google Translate)\n",
        "    * Chatbots & Voice Assistants\n",
        "    * Sentiment Analysis\n",
        "    * Text Summarization\n",
        "    * Speech Recognition\n"
      ],
      "metadata": {
        "id": "z_rZ8gxDVYHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q2. Briefly describe the historical evolution of Natural Language Processing (NLP)\n",
        "\n",
        "  The evolution of **Natural Language Processing (NLP)** can be broadly divided into **four major phases**, each driven by advances in computing power, data availability, and algorithms.\n",
        "\n",
        "\n",
        "  ### **1. Rule-Based Era (1950s–1970s)**\n",
        "\n",
        "  * Early NLP systems relied on **handcrafted linguistic rules and grammar**.\n",
        "  * Language processing was done using **if–else rules, dictionaries, and syntax trees**.\n",
        "  * Example tasks: basic machine translation and grammar checking.\n",
        "  * **Limitation:** Rigid rules, hard to scale, and poor handling of ambiguity.\n",
        "\n",
        "  ### **2. Statistical NLP Era (1980s–2000s)**\n",
        "\n",
        "  * Shift from rules to **probability and statistics**.\n",
        "  * Models learned patterns from **large text corpora**.\n",
        "  * Techniques included **n-grams, Hidden Markov Models (HMMs), and probabilistic parsers**.\n",
        "  * **Advantage:** Better handling of uncertainty and real-world language usage.\n",
        "\n",
        "\n",
        "  ### **3. Machine Learning Era (2000s–2010s)**\n",
        "\n",
        "  * Introduction of **supervised and unsupervised learning**.\n",
        "  * Algorithms like **Naive Bayes, SVMs, Decision Trees** became common.\n",
        "  * NLP tasks such as **spam detection, sentiment analysis, and text classification** improved significantly.\n",
        "  * **Limitation:** Heavy feature engineering was required.\n",
        "\n",
        "\n",
        "  ### **4. Deep Learning & Transformer Era (2015–Present)**\n",
        "\n",
        "  * Use of **neural networks**, especially **RNNs, LSTMs, and Transformers**.\n",
        "  * Models automatically learn features from data.\n",
        "  * Enabled high performance in **translation, summarization, question answering, and chatbots**.\n",
        "  * **Advantage:** State-of-the-art accuracy with minimal manual feature design.\n"
      ],
      "metadata": {
        "id": "Qtyso48VWV-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q3. Three Major Use Cases of NLP in Today’s Tech Industry\n",
        "\n",
        "\n",
        "  **1. Chatbots and Virtual Assistants**\n",
        "\n",
        "  NLP enables systems to understand user queries, identify intent, and generate appropriate responses. These systems are widely used in customer support, banking, and e-commerce to provide instant and automated assistance.\n",
        "\n",
        "  **Example:** Customer support chatbots, voice assistants.\n",
        "\n",
        "\n",
        "  **2. Sentiment Analysis**\n",
        "\n",
        "  Sentiment analysis uses NLP techniques to determine the emotional tone of text such as positive, negative, or neutral. It is commonly used to analyze customer reviews, social media posts, and feedback to understand user opinions.\n",
        "\n",
        "  **Example:** Brand monitoring, product review analysis.\n",
        "\n",
        "\n",
        "  **3. Machine Translation**\n",
        "\n",
        "  NLP allows automatic translation of text from one language to another while preserving meaning and context. This helps businesses and users communicate across different languages.\n",
        "\n",
        "  **Example:** Language translation tools, multilingual websites.\n",
        "\n"
      ],
      "metadata": {
        "id": "0Q9ZhNn1W8U2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. What is Text Normalization and why is it essential in text processing tasks?\n",
        "\n",
        "  **Text normalization** is the process of **cleaning and standardizing raw text** into a consistent format so that it can be effectively processed by NLP algorithms. It transforms text into a uniform representation by reducing variations that do not add meaningful information.\n",
        "\n",
        "\n",
        "  ### **Common Text Normalization Techniques**\n",
        "\n",
        "  * Converting text to **lowercase**\n",
        "  * Removing **punctuation, special characters, and extra spaces**\n",
        "  * **Tokenization** (splitting text into words)\n",
        "  * Removing **stop words** (e.g., *is, the, and*)\n",
        "  * **Stemming** or **lemmatization** (reducing words to their root form)\n",
        "  * Expanding **contractions** (e.g., *don’t → do not*)\n",
        "\n",
        "\n",
        "  ### **Why Text Normalization is Essential**\n",
        "\n",
        "  * **Improves model accuracy** by reducing noise and inconsistencies\n",
        "  * **Ensures uniform representation** of words (e.g., *Run, running, runs → run*)\n",
        "  * **Reduces vocabulary size**, making models more efficient\n",
        "  * **Enhances feature extraction** for machine learning algorithms\n",
        "  * **Improves consistency** across large datasets\n",
        "\n",
        "\n",
        "  ### **Example**\n",
        "\n",
        "  Raw text:\n",
        "\n",
        "  > *“Running, runs, and RUN!”*\n",
        "\n",
        "  After normalization:\n",
        "\n",
        "  > *“run”*\n",
        "\n"
      ],
      "metadata": {
        "id": "x77Cd4kHXhBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q5. Compare and contrast Stemming and Lemmatization with suitable examples\n",
        "\n",
        "  **Stemming** and **Lemmatization** are text normalization techniques used in NLP to reduce words to their base form, but they differ in approach and accuracy.\n",
        "\n",
        "\n",
        "  ### **Stemming**\n",
        "\n",
        "  * Reduces words by **removing suffixes** using simple rules.\n",
        "  * Does **not consider grammar or word meaning**.\n",
        "  * Output may **not be a valid dictionary word**.\n",
        "  * Faster but less accurate.\n",
        "\n",
        "  **Example:**\n",
        "\n",
        "  * *running → run*\n",
        "  * *studies → studi*\n",
        "  * *better → bett*\n",
        "\n",
        "\n",
        "  ### **Lemmatization**\n",
        "\n",
        "  * Reduces words to their **dictionary base form (lemma)**.\n",
        "  * Considers **part of speech and context**.\n",
        "  * Output is always a **valid word**.\n",
        "  * Slower but more accurate.\n",
        "\n",
        "  **Example:**\n",
        "\n",
        "  * *running → run*\n",
        "  * *studies → study*\n",
        "  * *better → good*\n",
        "\n",
        "\n",
        "  ### **Comparison Table**\n",
        "\n",
        "  | Aspect        | Stemming                  | Lemmatization                    |\n",
        "  | ------------- | ------------------------- | -------------------------------- |\n",
        "  | Approach      | Rule-based suffix removal | Dictionary + linguistic analysis |\n",
        "  | Accuracy      | Lower                     | Higher                           |\n",
        "  | Speed         | Faster                    | Slower                           |\n",
        "  | Output        | May be invalid word       | Always valid word                |\n",
        "  | POS awareness | No                        | Yes                              |\n"
      ],
      "metadata": {
        "id": "5n7Ci_wbYVFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q6. Write a Python program that uses regular expressions (regex) to extract all email addresses from the following block of text:\n",
        "\n",
        "“Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.”\n",
        "'''\n",
        "\n",
        "import re\n",
        "\n",
        "text = \"\"\"\n",
        "Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.\n",
        "\"\"\"\n",
        "\n",
        "pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "emails = re.findall(pattern, text)\n",
        "print(emails)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnMT2B_OZBMu",
        "outputId": "cacfd317-bf24-4d6b-b722-5cfd589f2ea5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@xyz.com', 'hr@xyz.com', 'john.doe@xyz.org', 'jenny_clarke126@mail.co.us', 'partners@xyz.biz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q7. Given the sample paragraph below, perform string tokenization and frequency distribution using Python and NLTK:\n",
        "\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.”\n",
        "'''\n",
        "\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\n",
        "\"\"\"\n",
        "\n",
        "text = text.lower()\n",
        "tokens = word_tokenize(text)\n",
        "tokens = [word for word in tokens if word not in string.punctuation]\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "print(freq_dist)\n",
        "\n",
        "print(\"\\nTop 10 Most Common Words:\")\n",
        "print(freq_dist.most_common(10))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK8C5AJ8ZPak",
        "outputId": "20395100-06cc-484a-e99c-5ed4c4fe8452"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', 'computer', 'science', 'and', 'artificial', 'intelligence', 'it', 'enables', 'machines', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'applications', 'of', 'nlp', 'include', 'chatbots', 'sentiment', 'analysis', 'and', 'machine', 'translation', 'as', 'technology', 'advances', 'the', 'role', 'of', 'nlp', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical']\n",
            "\n",
            "Frequency Distribution:\n",
            "<FreqDist with 43 samples and 50 outcomes>\n",
            "\n",
            "Top 10 Most Common Words:\n",
            "[('nlp', 3), ('and', 3), ('language', 2), ('is', 2), ('of', 2), ('natural', 1), ('processing', 1), ('a', 1), ('fascinating', 1), ('field', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q8.  Create a custom annotator using spaCy or NLTK that identifies and labels proper nouns in a given text.\n",
        "'''\n",
        "\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy.tokens import Span\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "@Language.component(\"proper_noun_annotator\")\n",
        "def proper_noun_annotator(doc):\n",
        "    spans = []\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"PROPN\":\n",
        "            span = Span(doc, token.i, token.i + 1, label=\"PROPER_NOUN\")\n",
        "            spans.append(span)\n",
        "    doc.ents = spans\n",
        "    return doc\n",
        "\n",
        "nlp.add_pipe(\"proper_noun_annotator\", last=True)\n",
        "\n",
        "text = \"John works at Google in New York and studies NLP at Stanford University.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \"->\", ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzrFVgqWZwXf",
        "outputId": "45e66206-a6b4-42ac-9a8c-28fe7066c906"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John -> PROPER_NOUN\n",
            "Google -> PROPER_NOUN\n",
            "New -> PROPER_NOUN\n",
            "York -> PROPER_NOUN\n",
            "NLP -> PROPER_NOUN\n",
            "Stanford -> PROPER_NOUN\n",
            "University -> PROPER_NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q9.  Using Genism, demonstrate how to train a simple Word2Vec model on the following dataset consisting of example sentences:\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar\n",
        "meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using\n",
        "Gensim.\n",
        "'''\n",
        "\n",
        "!pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFFbSmzZanuv",
        "outputId": "87fbf7a6-6cb8-4f7c-ac9d-c7950e420ad3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n"
      ],
      "metadata": {
        "id": "59qNpfL1b7Vd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGpY6h3lb-1f",
        "outputId": "998c6888-a20e-4aef-8492-3a44e465c3d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "processed_data = []\n",
        "\n",
        "for sentence in dataset:\n",
        "    sentence = sentence.lower()\n",
        "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(sentence)\n",
        "    processed_data.append(tokens)\n",
        "print(processed_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx0fb5F_cDlp",
        "outputId": "058bfb22-1ad1-47c0-c5ae-de9c0c6e3c76"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['natural', 'language', 'processing', 'enables', 'computers', 'to', 'understand', 'human', 'language'], ['word', 'embeddings', 'are', 'a', 'type', 'of', 'word', 'representation', 'that', 'allows', 'words', 'with', 'similar', 'meaning', 'to', 'have', 'similar', 'representation'], ['word2vec', 'is', 'a', 'popular', 'word', 'embedding', 'technique', 'used', 'in', 'many', 'nlp', 'applications'], ['text', 'preprocessing', 'is', 'a', 'critical', 'step', 'before', 'training', 'word', 'embeddings'], ['tokenization', 'and', 'normalization', 'help', 'clean', 'raw', 'text', 'for', 'modeling']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(\n",
        "    sentences=processed_data,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")\n"
      ],
      "metadata": {
        "id": "k9-8GknScO8w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = model.wv['language']\n",
        "print(\"Vector size:\", len(vector))\n",
        "\n",
        "similar_words = model.wv.most_similar('word', topn=3)\n",
        "print(similar_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZI3etNwcRwu",
        "outputId": "b818ec33-035d-41be-9b1c-abae8ba706ff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector size: 100\n",
            "[('tokenization', 0.21880948543548584), ('modeling', 0.21611614525318146), ('embedding', 0.19551844894886017)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q10. Imagine you are a data scientist at a fintech startup. You’ve been tasked with analyzing customer feedback. Outline the steps you would take to clean, process,and extract useful insights using NLP techniques from thousands of customer reviews.\n",
        "\n",
        "\n",
        "\n",
        "  As a data scientist analyzing **thousands of customer reviews**, the goal is to **clean text, process it, and extract actionable insights** such as common issues, customer sentiment, and frequently discussed topics.\n",
        "\n",
        "\n",
        "  ## **Step-by-Step NLP Workflow**\n",
        "\n",
        "  ### **1. Data Collection**\n",
        "\n",
        "  * Collect customer reviews from app stores, emails, surveys, or support tickets.\n",
        "\n",
        "  ### **2. Text Cleaning**\n",
        "\n",
        "  * Convert text to lowercase\n",
        "  * Remove punctuation, numbers, and special characters\n",
        "  * Remove stopwords (e.g., *is, the, and*)\n",
        "\n",
        "  ### **3. Tokenization**\n",
        "\n",
        "  * Split text into individual words (tokens)\n",
        "\n",
        "  ### **4. Normalization**\n",
        "\n",
        "  * Apply lemmatization to reduce words to base form\n",
        "\n",
        "  ### **5. Feature Extraction**\n",
        "\n",
        "  * Use word frequency or TF-IDF to identify important terms\n",
        "\n",
        "  ### **6. Sentiment Analysis**\n",
        "\n",
        "  * Classify reviews as **positive, negative, or neutral**\n",
        "\n",
        "  ### **7. Insight Generation**\n",
        "\n",
        "  * Identify common complaints, popular features, and overall customer satisfaction\n"
      ],
      "metadata": {
        "id": "X5-Im-rDcywK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "reviews = [\n",
        "    \"The app is very easy to use and transactions are super fast!\",\n",
        "    \"Customer support is slow and the app crashes frequently.\",\n",
        "    \"Great experience with instant loan approval.\",\n",
        "    \"Poor service, my payment failed multiple times.\",\n",
        "    \"User interface is clean but login issues are frustrating.\"\n",
        "]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "processed_tokens = []\n",
        "sentiment_results = []\n",
        "\n",
        "for review in reviews:\n",
        "    review = review.lower()\n",
        "    review = review.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(review)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    processed_tokens.extend(tokens)\n",
        "    sentiment = sia.polarity_scores(review)\n",
        "    sentiment_results.append(sentiment)\n",
        "freq_dist = FreqDist(processed_tokens)\n",
        "\n",
        "print(\"Most Common Words:\")\n",
        "print(freq_dist.most_common(10))\n",
        "\n",
        "print(\"\\nSentiment Scores:\")\n",
        "for score in sentiment_results:\n",
        "    print(score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3prWXMejdjCU",
        "outputId": "c73b2b32-ff63-4b3d-81e2-e45c0453b0d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Common Words:\n",
            "[('app', 2), ('easy', 1), ('use', 1), ('transaction', 1), ('super', 1), ('fast', 1), ('customer', 1), ('support', 1), ('slow', 1), ('crash', 1)]\n",
            "\n",
            "Sentiment Scores:\n",
            "{'neg': 0.0, 'neu': 0.585, 'pos': 0.415, 'compound': 0.796}\n",
            "{'neg': 0.0, 'neu': 0.748, 'pos': 0.252, 'compound': 0.4019}\n",
            "{'neg': 0.0, 'neu': 0.357, 'pos': 0.643, 'compound': 0.802}\n",
            "{'neg': 0.561, 'neu': 0.439, 'pos': 0.0, 'compound': -0.7506}\n",
            "{'neg': 0.303, 'neu': 0.551, 'pos': 0.146, 'compound': -0.4588}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VHvlK0wzd2xt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}